# Sistema RAG Completo - DocumentaciÃ³n

Sistema completo de Retrieval-Augmented Generation (RAG) para procesamiento, indexaciÃ³n y bÃºsqueda de documentos tÃ©cnicos.

## ğŸ“š Ãndice de DocumentaciÃ³n

### MÃ³dulos Principales

1. **[README_MODULE.md](README_MODULE.md)** - Parser de documentos PDF (Nemotron)
2. **[README_CHUNKER.md](README_CHUNKER.md)** - Sistema de chunking inteligente
3. **[README_EMBEDDINGS.md](README_EMBEDDINGS.md)** - GeneraciÃ³n de embeddings (BGE-M3)
4. **[README_VECTORSTORE.md](README_VECTORSTORE.md)** - Base de datos vectorial (ChromaDB)
5. **[README_RERANKING.md](README_RERANKING.md)** - Sistema de reranking (BGE-reranker)

## ğŸš€ Quick Start

### InstalaciÃ³n

```bash
# Clonar repositorio
cd Proyectos/20251223_Norm

# Crear entorno virtual
python -m venv venv

# Activar entorno virtual
# Windows PowerShell:
.\venv\Scripts\Activate.ps1
# Linux/Mac:
source venv/bin/activate

# Instalar dependencias
pip install -r requirements.txt
```

### Pipeline Completo

```python
# 1. PARSEAR DOCUMENTO PDF
from nemotron_parser import NemotronParser

parser = NemotronParser()
parser.process_pdf("documento.pdf", "output_simple/mi_doc")
# Genera: documento_concatenado.md

# 2. DIVIDIR EN CHUNKS
from document_chunker import DocumentChunker

chunker = DocumentChunker(
    chunk_size=2000,
    overlap=200,
    strategy="hybrid_semantic"
)
chunks = chunker.chunk_document(
    "output_simple/mi_doc/documento_concatenado.md",
    output_dir="chunks/"
)
# Genera: chunks_json/*.json

# 3. GENERAR EMBEDDINGS
from embedding_generator import EmbeddingGenerator

generator = EmbeddingGenerator("bge-m3")
generator.process_chunks_directory(
    chunks_dir="chunks/",
    output_dir="embeddings/"
)
# Genera: embeddings/*.json + embeddings.npy

# 4. INDEXAR EN CHROMADB
from vector_store import VectorStore

store = VectorStore(persist_directory="chroma_db")
store.add_embeddings_from_directory("embeddings/")
# Crea: chroma_db/

# 5. BUSCAR CON RERANKING
from reranker import Reranker

reranker = Reranker("bge-reranker-v2-m3")

# Query
query = "Â¿CuÃ¡les son las especificaciones de la turbina?"
query_emb = generator.generate_embedding(query)

# BÃºsqueda inicial
results = store.query_by_embedding(query_emb, n_results=20)

# Reranking
final = reranker.rerank_results(query, results, top_k=5)

# Mostrar resultados
for i, r in enumerate(final, 1):
    print(f"{i}. {r['id']} (score: {r['rerank_score']:.4f})")
    print(f"   {r['document'][:100]}...")
```

## ğŸ“¦ Estructura del Proyecto

```
20251223_Norm/
â”œâ”€â”€ # MÃ³dulos principales
â”œâ”€â”€ parse_local.py              # Parser PDF â†’ Markdown
â”œâ”€â”€ document_chunker.py         # Chunking inteligente
â”œâ”€â”€ embedding_generator.py      # GeneraciÃ³n embeddings
â”œâ”€â”€ vector_store.py             # ChromaDB wrapper
â”œâ”€â”€ reranker.py                 # Reranking cross-encoder
â”‚
â”œâ”€â”€ # Scripts de ejemplo
â”œâ”€â”€ ejemplos_chunker.py         # Ejemplos de chunking
â”œâ”€â”€ ejemplos_embeddings.py      # Ejemplos embeddings
â”œâ”€â”€ ejemplos_vector_store.py    # Ejemplos ChromaDB
â”œâ”€â”€ ejemplos_reranking.py       # Ejemplos reranking
â”‚
â”œâ”€â”€ # Tests
â”œâ”€â”€ test_embeddings_install.py  # Verificar instalaciÃ³n
â”œâ”€â”€ test_embeddings_generated.py # Verificar embeddings
â”‚
â”œâ”€â”€ # DocumentaciÃ³n
â”œâ”€â”€ README.md                   # Este archivo
â”œâ”€â”€ README_MODULE.md            # Doc parser
â”œâ”€â”€ README_CHUNKER.md           # Doc chunking
â”œâ”€â”€ README_EMBEDDINGS.md        # Doc embeddings
â”œâ”€â”€ README_VECTORSTORE.md       # Doc ChromaDB
â”œâ”€â”€ README_RERANKING.md         # Doc reranking
â”‚
â”œâ”€â”€ # Outputs
â”œâ”€â”€ output_simple/              # PDFs parseados
â”‚   â””â”€â”€ NREL5MW_Reduced/
â”‚       â”œâ”€â”€ documento_concatenado.md
â”‚       â”œâ”€â”€ chunks/             # Chunks markdown
â”‚       â””â”€â”€ chunks_json/        # Chunks JSON
â”‚
â””â”€â”€ output_rag/                 # Sistema RAG
    â”œâ”€â”€ embeddings/             # Embeddings generados
    â”‚   â”œâ”€â”€ chunk_*.json
    â”‚   â”œâ”€â”€ embeddings.npy
    â”‚   â””â”€â”€ embeddings_metadata.json
    â””â”€â”€ chroma_db/              # Base de datos vectorial
        â””â”€â”€ ...
```

## ğŸ¯ CaracterÃ­sticas Principales

### 1. Parser de Documentos
- ğŸ“„ Convierte PDF a Markdown estructurado
- ğŸ–¼ï¸ Extrae figuras y tablas
- ğŸ“Š Mantiene estructura del documento
- ğŸ¨ Genera visualizaciones con bounding boxes

### 2. Chunking Inteligente
- âœ‚ï¸ **3 estrategias**: Fixed, Semantic, Hybrid
- ğŸ”— **Overlap configurable** para contexto
- ğŸ“ **Control de tamaÃ±o** adaptativo
- ğŸ“Š **Metadata rica** en cada chunk

### 3. Embeddings
- ğŸ§  **BGE-M3**: 1024 dims, multilingÃ¼e
- âš¡ **GPU accelerated** (RTX 5080)
- ğŸ’¾ **MÃºltiples formatos**: JSON, NumPy
- ğŸ”„ **NormalizaciÃ³n** para cosine similarity

### 4. Vector Store
- ğŸ—„ï¸ **ChromaDB**: Persistente, rÃ¡pido
- ğŸ” **BÃºsqueda semÃ¡ntica** avanzada
- ğŸ¯ **Filtros** por metadata
- ğŸ“Š **MÃ©tricas**: Cosine, L2, IP

### 5. Reranking
- ğŸ¯ **BGE-reranker-v2-m3**: Cross-encoder
- ğŸ“ˆ **+15-20% precisiÃ³n** vs solo embeddings
- ğŸ”„ **AnÃ¡lisis de cambios** de ranking
- âš¡ **GPU optimized**

## ğŸ“Š Performance

### Hardware
- **GPU**: NVIDIA GeForce RTX 5080
- **CPU**: Compatible con cualquier sistema
- **RAM**: 8GB+ recomendado

### MÃ©tricas (24 chunks, documento NREL)

| OperaciÃ³n | Tiempo | Observaciones |
|-----------|--------|---------------|
| Parsing PDF | ~30s | Por documento |
| Chunking | <1s | 24 chunks generados |
| Embeddings | 0.25s | BGE-M3, GPU |
| Indexar ChromaDB | 0.5s | Primera carga |
| Query bÃ¡sica | 5-10ms | Top 10 resultados |
| Query + Reranking | ~150ms | Top 5 refinados |

### PrecisiÃ³n

| MÃ©todo | Recall@5 | Precision@5 | Observaciones |
|--------|----------|-------------|---------------|
| Embeddings solo | Base | Base | RÃ¡pido |
| + Reranking | +15-20% | +15-20% | MÃ¡s preciso |
| + Filtros | +10-15% | Variable | Depende filtros |

## ğŸ› ï¸ Ejemplos de Uso

### Ejemplo 1: BÃºsqueda simple

```bash
# Activar entorno
.\venv\Scripts\Activate.ps1

# Ejecutar ejemplo de vector store
python ejemplos_vector_store.py 2

# Output:
# ğŸ” Query: What is the blade design of the wind turbine?
# 
# ğŸ“Š Resultados encontrados: 3
# 
# 1. chunk_0016 (similarity: 0.6468)
#    ## 3 Blade Aerodynamic Properties...
```

### Ejemplo 2: Comparar con/sin reranking

```bash
python ejemplos_reranking.py 1

# Output muestra cambios de ranking:
# chunk_0008 â†‘6 posiciones
# chunk_0016 â†“4 posiciones
```

### Ejemplo 3: Pipeline completo

```bash
# Ver ejemplos/demos/pipeline_rag.py para pipeline integrado
python ejemplos_reranking.py 4
```

## ğŸ”§ ConfiguraciÃ³n

### Modelos Recomendados

```python
# Embeddings
EmbeddingGenerator("bge-m3")           # Recomendado: multilingÃ¼e, 1024 dims
EmbeddingGenerator("bge-base")         # Alternativa: inglÃ©s, 768 dims
EmbeddingGenerator("minilm")           # RÃ¡pido: 384 dims

# Reranking
Reranker("bge-reranker-v2-m3")        # Recomendado: multilingÃ¼e, max 8K tokens
Reranker("bge-reranker-base")         # RÃ¡pido: inglÃ©s, max 512 tokens
Reranker("ms-marco-small")            # Muy rÃ¡pido: inglÃ©s
```

### ParÃ¡metros TÃ­picos

```python
# Chunking
DocumentChunker(
    chunk_size=2000,        # 1500-3000 para documentos tÃ©cnicos
    overlap=200,            # 10-20% del chunk_size
    strategy="hybrid_semantic"  # hybrid > semantic > fixed
)

# BÃºsqueda
store.query_by_embedding(
    query_embedding=emb,
    n_results=20,           # 3-5x lo que necesitas finalmente
    where={"length": {"$gt": 500}}  # Filtros opcionales
)

# Reranking
reranker.rerank_results(
    query=query,
    search_results=results,
    top_k=5                 # 3-10 tÃ­picamente
)
```

## ğŸ“š Recursos y Referencias

### Papers
- **BGE**: [C-Pack: Packaged Resources for General Chinese Embeddings](https://arxiv.org/abs/2309.07597)
- **RAG**: [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401)
- **ChromaDB**: [Chroma Documentation](https://docs.trychroma.com/)

### Tutoriales
- [Pinecone RAG Guide](https://www.pinecone.io/learn/retrieval-augmented-generation/)
- [LangChain RAG Tutorial](https://python.langchain.com/docs/use_cases/question_answering/)

### Modelos
- [BAAI BGE Models](https://huggingface.co/BAAI)
- [Sentence Transformers](https://www.sbert.net/)

## ğŸ¤ Casos de Uso

### âœ… Ideal para:
- ğŸ“š Sistemas Q&A sobre documentaciÃ³n tÃ©cnica
- ğŸ” BÃºsqueda semÃ¡ntica en corpus grandes
- ğŸ“– Asistentes de lectura de manuales
- ğŸ“ Herramientas educativas con material extenso
- ğŸ¢ Knowledge bases corporativas

### ğŸ¯ Tu caso: NREL 5MW Wind Turbine
- âœ… 24 chunks de especificaciones tÃ©cnicas
- âœ… BÃºsquedas sobre diseÃ±o de palas, torre, capacidad
- âœ… Sistema funcionando con alta precisiÃ³n
- âœ… Listo para integrar con LLM

## ğŸš§ PrÃ³ximos Pasos (Opcional)

### IntegraciÃ³n con LLM
```python
# Ejemplo con OpenAI
import openai

# Construir prompt
context = build_context_from_reranked(final_results)
prompt = f"""BasÃ¡ndote en el siguiente contexto, responde la pregunta.

Contexto:
{context}

Pregunta: {query}

Respuesta:"""

response = openai.ChatCompletion.create(
    model="gpt-4",
    messages=[{"role": "user", "content": prompt}]
)

print(response.choices[0].message.content)
```

### WebApp con Streamlit
```python
import streamlit as st

st.title("Sistema RAG - NREL 5MW Turbine")

query = st.text_input("Haz una pregunta:")

if st.button("Buscar"):
    # Tu pipeline RAG aquÃ­
    results = rag_pipeline(query)
    
    for r in results:
        st.write(f"**{r['id']}** (score: {r['score']:.4f})")
        st.write(r['document'])
        st.divider()
```

## â“ FAQ

**P: Â¿Necesito GPU?**  
R: No es obligatoria, pero acelera 10-50x los embeddings y reranking.

**P: Â¿Puedo usar con otros idiomas?**  
R: SÃ­, BGE-M3 y BGE-reranker-v2-m3 son multilingÃ¼es.

**P: Â¿Funciona con PDFs escaneados?**  
R: SÃ­, pero necesitas OCR previo. Nemotron parser funciona con PDFs nativos.

**P: Â¿CuÃ¡ntos chunks puedo indexar?**  
R: ChromaDB escala a millones. Con 10K chunks funciona perfectamente en laptop.

**P: Â¿Es necesario el reranking?**  
R: Para <50 chunks no es crÃ­tico. Para >100 chunks sÃ­ mejora notablemente.

## ğŸ“ Changelog

### v1.0.0 (2026-01-02)
- âœ… Parser de PDFs con Nemotron
- âœ… Sistema de chunking con 3 estrategias
- âœ… GeneraciÃ³n de embeddings con BGE-M3
- âœ… Vector store con ChromaDB
- âœ… Reranking con BGE-reranker-v2-m3
- âœ… DocumentaciÃ³n completa de todos los mÃ³dulos
- âœ… Scripts de ejemplo para cada componente

## ğŸ“„ Licencia

Este proyecto es para uso educativo y de investigaciÃ³n.

---

**Creado**: 2026-01-02  
**VersiÃ³n**: 1.0.0  
**Autor**: Sistema RAG  
**Contacto**: [Tu contacto aquÃ­]
