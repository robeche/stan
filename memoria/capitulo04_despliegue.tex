\chapter{Despliegue y Configuración}
%================================================

\section{Requisitos del Sistema}

\subsection{Hardware}

\begin{table}[h]
\centering
\begin{tabular}{|l|l|}
\hline
\textbf{Componente} & \textbf{Especificación Recomendada} \\
\hline
CPU & 8+ cores \\
RAM & 16 GB mínimo, 32 GB recomendado \\
GPU & NVIDIA con 12+ GB VRAM (RTX 3060+) \\
Almacenamiento & 50 GB SSD \\
\hline
\end{tabular}
\caption{Requisitos de hardware}
\end{table}

\subsection{Software}

\begin{itemize}
    \item Python 3.12+
    \item CUDA 12.0+ (para GPU)
    \item Redis Server
    \item Ollama (Docker o instalación local)
\end{itemize}

\section{Instalación}

\subsection{1. Clonar Repositorio}

\begin{lstlisting}[language=bash]
git clone <repository_url>
cd WebApp
\end{lstlisting}

\subsection{2. Crear Entorno Virtual}

\begin{lstlisting}[language=bash]
python -m venv venv
source venv/bin/activate  # Linux/Mac
venv\Scripts\activate     # Windows
\end{lstlisting}

\subsection{3. Instalar Dependencias}

\begin{lstlisting}[language=bash]
pip install -r requirements.txt
\end{lstlisting}

\subsection{4. Configurar Base de Datos}

\begin{lstlisting}[language=bash]
python manage.py makemigrations
python manage.py migrate
python manage.py createsuperuser
\end{lstlisting}

\subsection{5. Iniciar Redis}

\begin{lstlisting}[language=bash]
redis-server
\end{lstlisting}

\subsection{6. Iniciar Celery Worker}

\begin{lstlisting}[language=bash]
celery -A rag_project worker -l info --pool=solo
\end{lstlisting}

\subsection{7. Iniciar Ollama}

\begin{lstlisting}[language=bash]
# Docker
docker run -d -p 11434:11434 --gpus all ollama/ollama
docker exec -it <container_id> ollama pull gpt-oss:20b

# O instalacion local
ollama serve
ollama pull gpt-oss:20b
\end{lstlisting}

\subsection{8. Iniciar Django}

\begin{lstlisting}[language=bash]
python manage.py runserver
\end{lstlisting}

\section{Configuración}

\subsection{Settings de Django}

\begin{lstlisting}[language=Python, caption=rag\_project/settings.py]
# RAG Configuration
RAG_CONFIG = {
    'PARSING': {
        'MODEL': 'nvidia/NVIDIA-Nemotron-Parse-v1.1',
        'DEVICE': 'cuda',
    },
    'CHUNKING': {
        'CHUNK_SIZE': 1200,
        'CHUNK_OVERLAP': 150,
        'STRATEGY': 'semantic',
    },
    'EMBEDDINGS': {
        'MODEL': 'BAAI/bge-m3',
        'DIMENSION': 1024,
        'DEVICE': 'cuda',
    },
    'VECTOR_STORE': {
        'TYPE': 'chromadb',
        'COLLECTION_NAME': 'rag_documents',
        'PERSIST_DIRECTORY': os.path.join(BASE_DIR, 'chroma_db'),
    }
}

# Ollama Configuration
OLLAMA_CONFIG = {
    'URL': 'http://localhost:11434',
    'MODEL': 'gpt-oss:20b',
    'TEMPERATURE': 0.7,
    'TOP_P': 0.9,
    'TIMEOUT': 60,
}

# Celery Configuration
CELERY_BROKER_URL = 'redis://localhost:6379/0'
CELERY_RESULT_BACKEND = 'redis://localhost:6379/0'
\end{lstlisting}

%================================================
