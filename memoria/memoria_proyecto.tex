\documentclass[12pt,a4paper]{report}
\usepackage[utf8]{inputenc}
\usepackage[spanish]{babel}
\usepackage{graphicx}
\usepackage{tikz}
\usetikzlibrary{shapes.geometric, arrows.meta, positioning, fit, backgrounds}
\usepackage{listingsutf8}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{geometry}
\usepackage{fancyhdr}
\usepackage{titlesec}
\usepackage{tcolorbox}
\usepackage{amssymb}

\geometry{margin=2.5cm}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
}

% Configuración de código
\lstset{
    basicstyle=\ttfamily\footnotesize,
    breaklines=true,
    frame=single,
    backgroundcolor=\color{gray!10},
    keywordstyle=\color{blue},
    commentstyle=\color{green!60!black},
    stringstyle=\color{red},
    inputencoding=utf8/latin1,
}

% Headers y footers
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{\leftmark}
\fancyhead[R]{\thepage}
\renewcommand{\headrulewidth}{0.5pt}

\title{
    \Huge\textbf{Sistema RAG con Django} \\
    \vspace{0.5cm}
    \Large Aplicación Web de Procesamiento \\
    y Consulta de Documentos Técnicos
    % \vspace{1cm}
    % \includegraphics[width=0.3\textwidth]{logo_placeholder.png}
}
\author{Roberto}
\date{\today}

\begin{document}

\maketitle

\tableofcontents
\newpage

%================================================
\chapter{Introducción}
%================================================

\section{Resumen Ejecutivo}

Este proyecto implementa un sistema completo de \textbf{Retrieval-Augmented Generation (RAG)} utilizando Django como framework web. El sistema permite procesar documentos técnicos complejos, extraer información estructurada, y proporcionar una interfaz de chatbot inteligente para consultar la información mediante procesamiento de lenguaje natural.

\subsection{Características Principales}

\begin{itemize}
    \item \textbf{Procesamiento de documentos}: Parsing automático con Nemotron Parse v1.1
    \item \textbf{Extracción inteligente}: Imágenes, tablas, y texto estructurado
    \item \textbf{Chunking semántico}: División contextual del contenido
    \item \textbf{Embeddings de alto rendimiento}: BGE-M3 con GPU (CUDA)
    \item \textbf{Búsqueda vectorial}: ChromaDB para retrieval eficiente
    \item \textbf{Chatbot conversacional}: Respuestas basadas en contexto con Ollama
    \item \textbf{Procesamiento asíncrono}: Celery + Redis para tareas en background
    \item \textbf{Interfaz administrativa}: Panel completo de gestión
\end{itemize}

\section{Objetivos del Sistema}

\begin{enumerate}
    \item Automatizar el procesamiento de documentos técnicos complejos
    \item Proporcionar acceso rápido a información mediante búsqueda semántica
    \item Generar respuestas contextuales en lenguaje natural
    \item Mantener trazabilidad de fuentes y referencias
    \item Soportar múltiples idiomas de forma transparente
\end{enumerate}

\section{Stack Tecnológico}

\begin{table}[h]
\centering
\begin{tabular}{|l|l|l|}
\hline
\textbf{Componente} & \textbf{Tecnología} & \textbf{Versión} \\
\hline
Framework Web & Django & 5.0 \\
Base de Datos & SQLite & 3.x \\
Task Queue & Celery & 5.x \\
Message Broker & Redis & Latest \\
Parsing & Nemotron Parse & v1.1 \\
Embeddings & BGE-M3 & sentence-transformers \\
Vector DB & ChromaDB & 0.5.23 \\
LLM & Ollama & gpt-oss:20b \\
GPU Support & CUDA & PyTorch \\
Frontend & Bootstrap & 5.3 \\
\hline
\end{tabular}
\caption{Stack tecnológico del proyecto}
\end{table}

%================================================
\chapter{Arquitectura del Sistema}
%================================================

\section{Visión General}

El sistema está compuesto por dos aplicaciones Django principales y varios módulos de procesamiento independientes.

\begin{figure}[h]
\centering
\begin{tikzpicture}[
    node distance=1.5cm,
    block/.style={rectangle, draw, fill=blue!20, text width=5em, text centered, rounded corners, minimum height=3em},
    service/.style={rectangle, draw, fill=green!20, text width=6em, text centered, rounded corners, minimum height=3em},
    storage/.style={cylinder, draw, fill=orange!20, shape border rotate=90, aspect=0.25, text width=4em, text centered, minimum height=3em},
    arrow/.style={-Stealth, thick}
]

% Frontend
\node[block] (user) {Usuario};

% Django Apps
\node[block, below=of user, xshift=-3cm] (admin) {Admin Panel};
\node[block, below=of user, xshift=3cm] (chatbot) {Chatbot};

% Django Core
\node[service, below=of user, yshift=-3cm] (django) {Django Core};

% Background Services
\node[service, below left=of django, yshift=-1cm] (celery) {Celery Worker};
\node[service, below right=of django, yshift=-1cm] (redis) {Redis};

% Processing Modules
\node[block, below=of celery, yshift=-1cm, xshift=-2cm] (parse) {Parsing\\(Nemotron)};
\node[block, below=of celery, yshift=-1cm] (chunk) {Chunking};
\node[block, below=of celery, yshift=-1cm, xshift=2cm] (embed) {Embeddings\\(BGE-M3)};

% Storage
\node[storage, below=of chunk, yshift=-2cm, xshift=-2cm] (sqlite) {SQLite};
\node[storage, below=of chunk, yshift=-2cm, xshift=2cm] (chroma) {ChromaDB};

% LLM
\node[service, right=of chatbot, xshift=1cm] (ollama) {Ollama\\LLM};

% Arrows
\draw[arrow] (user) -- (admin);
\draw[arrow] (user) -- (chatbot);
\draw[arrow] (admin) -- (django);
\draw[arrow] (chatbot) -- (django);
\draw[arrow] (django) -- (celery);
\draw[arrow] (django) -- (redis);
\draw[arrow] (celery) -- (parse);
\draw[arrow] (parse) -- (chunk);
\draw[arrow] (chunk) -- (embed);
\draw[arrow] (embed) -- (chroma);
\draw[arrow] (celery) -- (sqlite);
\draw[arrow] (chatbot) -- (chroma);
\draw[arrow] (chatbot) -- (ollama);

\end{tikzpicture}
\caption{Arquitectura general del sistema}
\end{figure}

\section{Flujo de Procesamiento de Documentos}

\begin{figure}[h]
\centering
\begin{tikzpicture}[
    node distance=2cm,
    stage/.style={rectangle, draw, fill=blue!30, text width=8em, text centered, rounded corners, minimum height=2.5em},
    decision/.style={diamond, draw, fill=yellow!30, text width=6em, text centered, aspect=2},
    arrow/.style={-Stealth, thick}
]

\node[stage] (upload) {1. Subida\\de Documento};
\node[stage, below=of upload] (parse) {2. Parsing\\(Nemotron)};
\node[stage, below=of parse] (extract) {3. Extracción\\Páginas/Imágenes/Tablas};
\node[stage, below=of extract] (chunk) {4. Chunking\\Semántico};
\node[stage, below=of chunk] (embed) {5. Generación\\Embeddings (GPU)};
\node[stage, below=of embed] (index) {6. Indexación\\ChromaDB};
\node[stage, below=of index] (complete) {7. Completado};

\draw[arrow] (upload) -- node[right] {Celery Task} (parse);
\draw[arrow] (parse) -- node[right] {JSON Output} (extract);
\draw[arrow] (extract) -- node[right] {Markdown + Media} (chunk);
\draw[arrow] (chunk) -- node[right] {Chunks Array} (embed);
\draw[arrow] (embed) -- node[right] {Vectors 1024D} (index);
\draw[arrow] (index) -- node[right] {Status Update} (complete);

\end{tikzpicture}
\caption{Pipeline de procesamiento de documentos}
\end{figure}

\section{Flujo de Consulta (Chatbot)}

\begin{figure}[h]
\centering
\begin{tikzpicture}[
    node distance=1.8cm,
    stage/.style={rectangle, draw, fill=green!30, text width=9em, text centered, rounded corners, minimum height=2.5em},
    arrow/.style={-Stealth, thick}
]

\node[stage] (query) {1. Pregunta del Usuario};
\node[stage, below=of query] (embed) {2. Embedding de la Query\\(BGE-M3)};
\node[stage, below=of embed] (search) {3. Búsqueda Vectorial\\(ChromaDB)};
\node[stage, below=of search] (retrieve) {4. Recuperar Top-K Chunks};
\node[stage, below=of retrieve] (context) {5. Construir Contexto\\+ Referencias};
\node[stage, below=of context] (llm) {6. Generación LLM\\(Ollama)};
\node[stage, below=of llm] (postprocess) {7. Post-Procesamiento\\Inyección de Imágenes};
\node[stage, below=of postprocess] (response) {8. Respuesta Final\\+ Fuentes};

\draw[arrow] (query) -- (embed);
\draw[arrow] (embed) -- node[right] {Vector 1024D} (search);
\draw[arrow] (search) -- node[right] {Chunk IDs} (retrieve);
\draw[arrow] (retrieve) -- node[right] {Chunks + Metadata} (context);
\draw[arrow] (context) -- node[right] {Prompt} (llm);
\draw[arrow] (llm) -- node[right] {Text + Referencias} (postprocess);
\draw[arrow] (postprocess) -- node[right] {HTML + Images} (response);

\end{tikzpicture}
\caption{Pipeline de consulta del chatbot}
\end{figure}

%================================================
\chapter{Módulos del Sistema}
%================================================

\section{Admin Panel}

\subsection{Funcionalidad}

El panel de administración proporciona una interfaz completa para gestionar el ciclo de vida de los documentos:

\begin{itemize}
    \item \textbf{Subida de documentos}: Soporta PDF, DOCX, DOC, TXT, MD
    \item \textbf{Monitoreo de progreso}: 4 etapas con porcentaje en tiempo real
    \item \textbf{Visualización de contenido}: Páginas anotadas, imágenes extraídas, tablas
    \item \textbf{Gestión de chunks}: Exploración de fragmentos indexados
    \item \textbf{Logs detallados}: Trazabilidad completa del procesamiento
    \item \textbf{Reprocesamiento}: Reindexar documentos con nuevos parámetros
\end{itemize}

\subsection{Modelos de Datos}

\begin{lstlisting}[language=Python, caption=Modelo Document]
class Document(models.Model):
    title = models.CharField(max_length=500)
    original_filename = models.CharField(max_length=500)
    file = models.FileField(upload_to='documents/\%Y/\%m/\%d/')
    status = models.CharField(max_length=20, choices=STATUS_CHOICES)
    progress_percentage = models.IntegerField(default=0)
    
    # Procesamiento
    parsing_completed = models.BooleanField(default=False)
    chunking_completed = models.BooleanField(default=False)
    embedding_completed = models.BooleanField(default=False)
    indexing_completed = models.BooleanField(default=False)
    
    # Estadisticas
    total_pages = models.IntegerField(default=0)
    total_chunks = models.IntegerField(default=0)
    total_images = models.IntegerField(default=0)
    total_tables = models.IntegerField(default=0)
\end{lstlisting}

\begin{lstlisting}[language=Python, caption=Modelo Chunk]
class Chunk(models.Model):
    document = models.ForeignKey(Document, on_delete=models.CASCADE)
    chunk_id = models.CharField(max_length=100)
    content = models.TextField()
    chunk_index = models.IntegerField()
    metadata = models.JSONField(default=dict)
    
    # Embedding
    embedding_vector = models.JSONField(null=True, blank=True)
    embedding_dimension = models.IntegerField(null=True, blank=True)
    
    # ChromaDB
    chromadb_id = models.CharField(max_length=255)
    indexed_in_chromadb = models.BooleanField(default=False)
\end{lstlisting}

\subsection{Tareas Celery}

\begin{lstlisting}[language=Python, caption=Tarea de procesamiento]
@shared_task(bind=True)
def process_document_task(self, document_id):
    # Procesa documento completo en background
    doc = Document.objects.get(id=document_id)
    
    # 1. Parsing (25 porciento)
    parse_result = parse_document(doc.file.path)
    update_progress(doc, 25, 'parsing')
    
    # 2. Chunking (50 porciento)
    chunks = chunk_document(parse_result)
    update_progress(doc, 50, 'chunking')
    
    # 3. Embeddings (75 porciento)
    embeddings = generate_embeddings(chunks)
    update_progress(doc, 75, 'embedding')
    
    # 4. Indexing (100 porciento)
    index_to_chromadb(chunks, embeddings)
    update_progress(doc, 100, 'completed')
\end{lstlisting}

\section{Chatbot}

\subsection{Funcionalidad}

Interfaz conversacional pública que permite:

\begin{itemize}
    \item \textbf{Consultas en lenguaje natural}: En español o inglés
    \item \textbf{Respuestas contextuales}: Basadas en los documentos indexados
    \item \textbf{Referencias visuales}: Tablas e imágenes embebidas automáticamente
    \item \textbf{Historial de conversación}: Sesiones independientes por usuario
    \item \textbf{Fuentes citadas}: Muestra los chunks utilizados
\end{itemize}

\subsection{Modelos de Datos}

\begin{lstlisting}[language=Python, caption=Modelo Conversation]
class Conversation(models.Model):
    session_id = models.CharField(max_length=100, unique=True)
    created_at = models.DateTimeField(auto_now_add=True)
    updated_at = models.DateTimeField(auto_now=True)

class Message(models.Model):
    MESSAGE_TYPES = [
        ('user', 'Usuario'),
        ('assistant', 'Asistente'),
    ]
    conversation = models.ForeignKey(Conversation)
    message_type = models.CharField(max_length=10, choices=MESSAGE_TYPES)
    content = models.TextField()
    retrieved_chunks = models.ManyToManyField('admin_panel.Chunk')
    created_at = models.DateTimeField(auto_now_add=True)
\end{lstlisting}

\subsection{Lógica de Respuesta}

\begin{lstlisting}[language=Python, caption=Generación de respuesta]
def generate_response(query):
    # 1. Generar embedding de la query
    query_embedding = embedding_generator.generate_single_embedding(query)
    
    # 2. Buscar en ChromaDB
    results = vector_store.query(
        query_embedding=query_embedding,
        n_results=5
    )
    
    # 3. Recuperar chunks
    chunk_ids = [r['metadata']['chunk_id'] for r in results['results']]
    chunks = Chunk.objects.filter(chunk_id__in=chunk_ids)
    
    # 4. Construir contexto
    context = build_context(chunks)
    
    # 5. Generar respuesta con LLM
    response = call_ollama(query, context)
    
    # 6. Post-procesar (inyectar imagenes)
    response = inject_media_references(response, chunks)
    
    return response, chunks
\end{lstlisting}

\section{Parsing (Nemotron)}

\subsection{Características}

Utiliza el modelo \texttt{nvidia/NVIDIA-Nemotron-Parse-v1.1} para:

\begin{itemize}
    \item Extracción de texto con estructura preservada
    \item Detección y extracción de imágenes con bounding boxes
    \item Identificación de tablas con metadatos
    \item Salida en formato Markdown + JSON
\end{itemize}

\subsection{Configuración}

\begin{lstlisting}[language=Python, caption=Configuración de Nemotron]
RAG_CONFIG = {
    'PARSING': {
        'MODEL': 'nvidia/NVIDIA-Nemotron-Parse-v1.1',
        'DEVICE': 'cuda' if torch.cuda.is_available() else 'cpu',
        'BATCH_SIZE': 4,
        'MAX_LENGTH': 512,
    }
}
\end{lstlisting}

\section{Chunking Semántico}

\subsection{Estrategia}

Implementa chunking basado en contexto semántico:

\begin{itemize}
    \item \textbf{Tamaño de chunk}: 1200 caracteres
    \item \textbf{Overlap}: 150 caracteres
    \item \textbf{Preservación de estructura}: Respeta párrafos y secciones
    \item \textbf{Metadatos}: Página, sección, documento, índice
\end{itemize}

\subsection{Implementación}

\begin{lstlisting}[language=Python, caption=Chunking semántico]
class DocumentChunker:
    def __init__(self, chunk_size=1200, chunk_overlap=150):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
    
    def chunk_document(self, text, metadata=None):
        chunks = []
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.chunk_size,
            chunk_overlap=self.chunk_overlap,
            separators=["\n\n", "\n", ". ", " ", ""]
        )
        
        for i, chunk_text in enumerate(splitter.split_text(text)):
            chunk = {
                'chunk_id': f'chunk_{i:04d}',
                'content': chunk_text,
                'chunk_index': i,
                'metadata': metadata or {}
            }
            chunks.append(chunk)
        
        return chunks
\end{lstlisting}

\section{Embeddings (BGE-M3)}

\subsection{Modelo}

Utiliza \texttt{BAAI/bge-m3} de SentenceTransformers:

\begin{itemize}
    \item \textbf{Dimensiones}: 1024
    \item \textbf{Backend}: sentence-transformers
    \item \textbf{Dispositivo}: CUDA (GPU)
    \item \textbf{Multilingüe}: Español, inglés, y más de 100 idiomas
\end{itemize}

\subsection{Implementación}

\begin{lstlisting}[language=Python, caption=Generador de embeddings]
class EmbeddingGenerator:
    def __init__(self, model_name='BAAI/bge-m3'):
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        self.model = SentenceTransformer(model_name, device=self.device)
        self.dimension = 1024
    
    def generate_embeddings(self, texts):
        # Genera embeddings para multiples textos
        embeddings = self.model.encode(
            texts,
            convert_to_numpy=True,
            show_progress_bar=True
        )
        return embeddings
    
    def generate_single_embedding(self, text):
        # Genera embedding para un solo texto
        embedding = self.model.encode(text, convert_to_numpy=True)
        return embedding.tolist()
\end{lstlisting}

\section{Vector Store (ChromaDB)}

\subsection{Configuración}

\begin{lstlisting}[language=Python, caption=Inicialización de ChromaDB]
class VectorStore:
    def __init__(self, collection_name='rag_documents', 
                 persist_directory='./chroma_db'):
        self.client = chromadb.PersistentClient(path=persist_directory)
        self.collection = self.client.get_or_create_collection(
            name=collection_name,
            metadata={"hnsw:space": "cosine"}
        )
    
    def add_documents(self, chunks, embeddings, metadatas, ids):
        # Aniade documentos a la coleccion
        self.collection.add(
            embeddings=embeddings,
            documents=chunks,
            metadatas=metadatas,
            ids=ids
        )
    
    def query(self, query_embedding, n_results=5):
        # Busca documentos similares
        results = self.collection.query(
            query_embeddings=[query_embedding],
            n_results=n_results,
            include=['documents', 'metadatas', 'distances']
        )
        return results
\end{lstlisting}

\section{LLM (Ollama)}

\subsection{Integración}

Utiliza Ollama como servidor LLM local:

\begin{itemize}
    \item \textbf{Modelo}: gpt-oss:20b
    \item \textbf{Puerto}: 11434
    \item \textbf{API}: HTTP REST
    \item \textbf{Timeout}: 60 segundos
\end{itemize}

\subsection{Prompt Engineering}

\begin{lstlisting}[caption=Prompt del sistema]
You are an expert assistant that answers questions based 
on technical documents.

**CRITICAL: Respond in the SAME LANGUAGE as the user's question.**

Document context:
{context}

User's question: {query}

Instructions:
- Answer clearly and concisely
- Use ONLY the information provided in the context
- If the information is not in the context, state it clearly
- Cite the document when relevant
- IMPORTANT: If you mention a table or image, include its 
  EXACT reference (example: TABLA_15 or IMAGEN_8)
\end{lstlisting}

%================================================
\chapter{Despliegue y Configuración}
%================================================

\section{Requisitos del Sistema}

\subsection{Hardware}

\begin{table}[h]
\centering
\begin{tabular}{|l|l|}
\hline
\textbf{Componente} & \textbf{Especificación Recomendada} \\
\hline
CPU & 8+ cores \\
RAM & 16 GB mínimo, 32 GB recomendado \\
GPU & NVIDIA con 12+ GB VRAM (RTX 3060+) \\
Almacenamiento & 50 GB SSD \\
\hline
\end{tabular}
\caption{Requisitos de hardware}
\end{table}

\subsection{Software}

\begin{itemize}
    \item Python 3.12+
    \item CUDA 12.0+ (para GPU)
    \item Redis Server
    \item Ollama (Docker o instalación local)
\end{itemize}

\section{Instalación}

\subsection{1. Clonar Repositorio}

\begin{lstlisting}[language=bash]
git clone <repository_url>
cd WebApp
\end{lstlisting}

\subsection{2. Crear Entorno Virtual}

\begin{lstlisting}[language=bash]
python -m venv venv
source venv/bin/activate  # Linux/Mac
venv\Scripts\activate     # Windows
\end{lstlisting}

\subsection{3. Instalar Dependencias}

\begin{lstlisting}[language=bash]
pip install -r requirements.txt
\end{lstlisting}

\subsection{4. Configurar Base de Datos}

\begin{lstlisting}[language=bash]
python manage.py makemigrations
python manage.py migrate
python manage.py createsuperuser
\end{lstlisting}

\subsection{5. Iniciar Redis}

\begin{lstlisting}[language=bash]
redis-server
\end{lstlisting}

\subsection{6. Iniciar Celery Worker}

\begin{lstlisting}[language=bash]
celery -A rag_project worker -l info --pool=solo
\end{lstlisting}

\subsection{7. Iniciar Ollama}

\begin{lstlisting}[language=bash]
# Docker
docker run -d -p 11434:11434 --gpus all ollama/ollama
docker exec -it <container_id> ollama pull gpt-oss:20b

# O instalacion local
ollama serve
ollama pull gpt-oss:20b
\end{lstlisting}

\subsection{8. Iniciar Django}

\begin{lstlisting}[language=bash]
python manage.py runserver
\end{lstlisting}

\section{Configuración}

\subsection{Settings de Django}

\begin{lstlisting}[language=Python, caption=rag\_project/settings.py]
# RAG Configuration
RAG_CONFIG = {
    'PARSING': {
        'MODEL': 'nvidia/NVIDIA-Nemotron-Parse-v1.1',
        'DEVICE': 'cuda',
    },
    'CHUNKING': {
        'CHUNK_SIZE': 1200,
        'CHUNK_OVERLAP': 150,
        'STRATEGY': 'semantic',
    },
    'EMBEDDINGS': {
        'MODEL': 'BAAI/bge-m3',
        'DIMENSION': 1024,
        'DEVICE': 'cuda',
    },
    'VECTOR_STORE': {
        'TYPE': 'chromadb',
        'COLLECTION_NAME': 'rag_documents',
        'PERSIST_DIRECTORY': os.path.join(BASE_DIR, 'chroma_db'),
    }
}

# Ollama Configuration
OLLAMA_CONFIG = {
    'URL': 'http://localhost:11434',
    'MODEL': 'gpt-oss:20b',
    'TEMPERATURE': 0.7,
    'TOP_P': 0.9,
    'TIMEOUT': 60,
}

# Celery Configuration
CELERY_BROKER_URL = 'redis://localhost:6379/0'
CELERY_RESULT_BACKEND = 'redis://localhost:6379/0'
\end{lstlisting}

%================================================
\chapter{Uso del Sistema}
%================================================

\section{Panel de Administración}

\subsection{Acceso}

\begin{enumerate}
    \item Navegar a \url{http://localhost:8000/admin-panel/}
    \item Iniciar sesión con credenciales de administrador
\end{enumerate}

\subsection{Subir Documento}

\begin{enumerate}
    \item Click en "Subir Documento"
    \item Seleccionar archivo (PDF, DOCX, DOC, TXT, MD)
    \item Ingresar título
    \item Click en "Procesar Documento"
    \item Monitorear progreso en dashboard
\end{enumerate}

\subsection{Explorar Contenido}

\begin{enumerate}
    \item Click en documento en el dashboard
    \item Ver páginas anotadas, imágenes, tablas
    \item Explorar chunks generados
    \item Revisar logs de procesamiento
\end{enumerate}

\section{Chatbot}

\subsection{Acceso}

\begin{enumerate}
    \item Navegar a \url{http://localhost:8000/}
    \item No requiere autenticación
\end{enumerate}

\subsection{Realizar Consultas}

\begin{enumerate}
    \item Escribir pregunta en el campo de texto
    \item Presionar Enter o click en "Enviar"
    \item Esperar respuesta (10-30 segundos primera vez)
    \item Ver respuesta con tablas/imágenes embebidas
    \item Revisar fuentes consultadas al final
\end{enumerate}

\subsection{Nueva Conversación}

\begin{enumerate}
    \item Click en "Nueva conversación"
    \item Se crea una nueva sesión independiente
\end{enumerate}

%================================================
\chapter{Diagramas de Secuencia}
%================================================

\section{Procesamiento de Documento}

\begin{figure}[h]
\centering
\begin{tikzpicture}[
    node distance=1cm,
    actor/.style={rectangle, draw, fill=blue!20, minimum width=3cm, minimum height=1cm},
    message/.style={-Stealth, thick}
]

% Actores
\node[actor] (admin) at (0,0) {Admin};
\node[actor] (django) at (4,0) {Django};
\node[actor] (celery) at (8,0) {Celery};
\node[actor] (modules) at (12,0) {Módulos RAG};

% Líneas de vida
\draw[dashed] (admin) -- ++(0,-12);
\draw[dashed] (django) -- ++(0,-12);
\draw[dashed] (celery) -- ++(0,-12);
\draw[dashed] (modules) -- ++(0,-12);

% Mensajes
\draw[message] (0,-1) -- node[above] {Upload} (4,-1);
\draw[message] (4,-2) -- node[above] {Create Task} (8,-2);
\draw[message] (8,-3) -- node[above] {Parse} (12,-3);
\draw[message] (12,-4) -- node[above] {Result} (8,-4);
\draw[message] (8,-5) -- node[above] {Chunk} (12,-5);
\draw[message] (12,-6) -- node[above] {Result} (8,-6);
\draw[message] (8,-7) -- node[above] {Embed} (12,-7);
\draw[message] (12,-8) -- node[above] {Result} (8,-8);
\draw[message] (8,-9) -- node[above] {Index} (12,-9);
\draw[message] (12,-10) -- node[above] {Result} (8,-10);
\draw[message] (8,-11) -- node[above] {Complete} (4,-11);

\end{tikzpicture}
\caption{Diagrama de secuencia: Procesamiento de documento}
\end{figure}

\section{Consulta en Chatbot}

\begin{figure}[h]
\centering
\begin{tikzpicture}[
    node distance=1cm,
    actor/.style={rectangle, draw, fill=green!20, minimum width=3cm, minimum height=1cm},
    message/.style={-Stealth, thick}
]

% Actores
\node[actor] (user) at (0,0) {Usuario};
\node[actor] (chatbot) at (4,0) {Chatbot};
\node[actor] (chroma) at (8,0) {ChromaDB};
\node[actor] (ollama) at (12,0) {Ollama};

% Líneas de vida
\draw[dashed] (user) -- ++(0,-10);
\draw[dashed] (chatbot) -- ++(0,-10);
\draw[dashed] (chroma) -- ++(0,-10);
\draw[dashed] (ollama) -- ++(0,-10);

% Mensajes
\draw[message] (0,-1) -- node[above] {Query} (4,-1);
\draw[message] (4,-2) -- node[above] {Generate Embedding} (4,-2.5);
\draw[message] (4,-3) -- node[above] {Search} (8,-3);
\draw[message] (8,-4) -- node[above] {Chunks} (4,-4);
\draw[message] (4,-5) -- node[above] {Build Context} (4,-5.5);
\draw[message] (4,-6) -- node[above] {Generate} (12,-6);
\draw[message] (12,-7) -- node[above] {Response} (4,-7);
\draw[message] (4,-8) -- node[above] {Post-process} (4,-8.5);
\draw[message] (4,-9) -- node[above] {Response + Media} (0,-9);

\end{tikzpicture}
\caption{Diagrama de secuencia: Consulta en chatbot}
\end{figure}

%================================================
\chapter{Rendimiento y Optimización}
%================================================

\section{Métricas de Rendimiento}

\begin{table}[h]
\centering
\begin{tabular}{|l|l|l|}
\hline
\textbf{Operación} & \textbf{Tiempo Promedio} & \textbf{Notas} \\
\hline
Parsing (10 páginas) & 2-3 minutos & GPU acelerado \\
Chunking & 5-10 segundos & \\
Embedding generation & 10-30 segundos & Batch de 30 chunks \\
ChromaDB indexing & 1-2 segundos & \\
Query embedding & 0.5-1 segundo & \\
Vector search & 0.1-0.5 segundos & \\
LLM generation & 10-30 segundos & Primera llamada \\
LLM generation & 3-5 segundos & Llamadas siguientes \\
\hline
\end{tabular}
\caption{Métricas de rendimiento}
\end{table}

\section{Optimizaciones Implementadas}

\begin{itemize}
    \item \textbf{GPU acceleration}: CUDA para embeddings y parsing
    \item \textbf{Batch processing}: Embeddings generados en lotes
    \item \textbf{Async processing}: Celery para tareas largas
    \item \textbf{Caching}: Redis para resultados intermedios
    \item \textbf{Lazy loading}: Imágenes cargadas bajo demanda
    \item \textbf{Connection pooling}: ChromaDB persistent client
\end{itemize}

\section{Recomendaciones}

\begin{enumerate}
    \item \textbf{Pre-cargar modelos}: Warm-up de BGE-M3 y Ollama al inicio
    \item \textbf{Monitoreo}: Usar Flower para Celery tasks
    \item \textbf{Logs}: Configurar logging estructurado
    \item \textbf{Backups}: Respaldar SQLite y ChromaDB periódicamente
    \item \textbf{Escalabilidad}: Considerar PostgreSQL para producción
\end{enumerate}

%================================================
\chapter{Resolución de Problemas}
%================================================

\section{Problemas Comunes}

\subsection{ChromaDB no encuentra documentos}

\textbf{Síntoma}: Query returns empty results

\textbf{Solución}:
\begin{enumerate}
    \item Verificar que documentos están indexados: \texttt{collection.count()}
    \item Revisar dimensión de embeddings (debe ser 1024)
    \item Comprobar nombres de colección coinciden
    \item Verificar \texttt{persist\_directory} es correcto
\end{enumerate}

\subsection{Celery task fails}

\textbf{Síntoma}: Processing stuck o failed

\textbf{Solución}:
\begin{enumerate}
    \item Revisar logs de Celery worker
    \item Verificar Redis está corriendo: \texttt{redis-cli ping}
    \item Comprobar permisos de archivos
    \item Revisar memoria GPU disponible
\end{enumerate}

\subsection{Ollama timeout}

\textbf{Síntoma}: LLM no responde o timeout

\textbf{Solución}:
\begin{enumerate}
    \item Verificar Ollama está corriendo: \texttt{curl http://localhost:11434}
    \item Aumentar timeout en settings
    \item Revisar logs de Ollama
    \item Considerar modelo más pequeño si GPU limitada
\end{enumerate}

\subsection{Out of GPU memory}

\textbf{Síntoma}: CUDA out of memory error

\textbf{Solución}:
\begin{enumerate}
    \item Reducir batch size de embeddings
    \item Cerrar otros procesos usando GPU
    \item Usar modelo más pequeño
    \item Considerar CPU fallback
\end{enumerate}

%================================================
\chapter{Conclusiones y Trabajo Futuro}
%================================================

\section{Logros del Proyecto}

\begin{itemize}
    \item [\checkmark] Pipeline RAG completo funcional
    \item [\checkmark] Procesamiento automatico de documentos
    \item [\checkmark] Busqueda vectorial eficiente
    \item [\checkmark] Chatbot conversacional multilingue
    \item [\checkmark] Integracion de tablas e imagenes
    \item [\checkmark] Interface administrativa completa
    \item [\checkmark] Procesamiento asincrono robusto
\end{itemize}

\section{Trabajo Futuro}

\subsection{Funcionalidades}

\begin{itemize}
    \item Reranking de resultados
    \item Multi-query fusion
    \item Feedback de usuarios sobre respuestas
    \item Exportar conversaciones a PDF
    \item API REST para integración externa
    \item Autenticación de usuarios para chatbot
    \item Dashboard de analytics
\end{itemize}

\subsection{Optimizaciones}

\begin{itemize}
    \item Quantización de modelos
    \item Cacheo de embeddings frecuentes
    \item Streaming de respuestas LLM
    \item Pre-fetching de imágenes
    \item Compresión de imágenes
\end{itemize}

\subsection{Escalabilidad}

\begin{itemize}
    \item Migración a PostgreSQL
    \item Implementar load balancing
    \item Kubernetes deployment
    \item Multi-tenancy support
    \item Distributed ChromaDB
\end{itemize}

\section{Reflexión Final}

Este proyecto demuestra la viabilidad de implementar un sistema RAG completo utilizando tecnologías open-source y modelos de última generación. La combinación de Django, ChromaDB, y Ollama proporciona una base sólida para aplicaciones de IA conversacional basadas en documentos.

La arquitectura modular permite fácil extensión y personalización, mientras que el procesamiento asíncrono garantiza una experiencia de usuario fluida incluso con documentos grandes.

%================================================
\appendix
\chapter{Apéndices}
%================================================

\section{Comandos Útiles}

\subsection{Django}

\begin{lstlisting}[language=bash]
# Crear migraciones
python manage.py makemigrations

# Aplicar migraciones
python manage.py migrate

# Crear superusuario
python manage.py createsuperuser

# Shell interactivo
python manage.py shell

# Colectar archivos estaticos
python manage.py collectstatic
\end{lstlisting}

\subsection{Celery}

\begin{lstlisting}[language=bash]
# Iniciar worker
celery -A rag_project worker -l info --pool=solo

# Monitorear con Flower
celery -A rag_project flower

# Purge all tasks
celery -A rag_project purge
\end{lstlisting}

\subsection{Redis}

\begin{lstlisting}[language=bash]
# Iniciar servidor
redis-server

# CLI
redis-cli

# Ping
redis-cli ping

# Flush all
redis-cli FLUSHALL
\end{lstlisting}

\section{Estructura de Directorios}

\begin{lstlisting}
WebApp/
  admin_panel/          # App de administracion
    models.py
    views.py
    tasks.py
    urls.py
    templates/
  chatbot/             # App de chatbot
    models.py
    views.py
    urls.py
    templates/
  rag_project/         # Configuracion Django
    settings.py
    urls.py
    celery.py
  tools/               # Modulos RAG
    document_chunker.py
    embedding_generator.py
    vector_store.py
    reranker.py
  media/               # Archivos subidos
    documents/
    extracted_images/
    extracted_tables/
  chroma_db/           # Base de datos vectorial
  templates/           # Templates globales
  manage.py
  requirements.txt
\end{lstlisting}

\section{Referencias}

\begin{itemize}
    \item Django Documentation: \url{https://docs.djangoproject.com/}
    \item ChromaDB Documentation: \url{https://docs.trychroma.com/}
    \item Sentence Transformers: \url{https://www.sbert.net/}
    \item Ollama Documentation: \url{https://ollama.ai/}
    \item Celery Documentation: \url{https://docs.celeryproject.org/}
    \item BGE-M3 Paper: \url{https://huggingface.co/BAAI/bge-m3}
    \item Nemotron Parse: \url{https://huggingface.co/nvidia/NVIDIA-Nemotron-Parse-v1.1}
\end{itemize}

\end{document}
