# M√≥dulo de Generaci√≥n de Embeddings para RAG

Sistema modular y flexible para generar embeddings vectoriales de documentos, optimizado para sistemas RAG (Retrieval-Augmented Generation).

## üéØ Caracter√≠sticas

- ‚úÖ **M√∫ltiples modelos soportados**: Nemotron, BGE, MiniLM, MPNet, OpenAI
- ‚úÖ **Backends flexibles**: Sentence Transformers (local) y OpenAI API
- ‚úÖ **GPU/CPU autom√°tico**: Detecci√≥n y uso autom√°tico de hardware disponible
- ‚úÖ **Procesamiento batch**: Optimizado para grandes vol√∫menes de datos
- ‚úÖ **Normalizaci√≥n**: Embeddings normalizados para cosine similarity
- ‚úÖ **M√∫ltiples formatos**: JSON, NumPy, o ambos
- ‚úÖ **Progreso visual**: Barras de progreso para procesos largos
- ‚úÖ **Metadata completa**: Informaci√≥n detallada de cada generaci√≥n

## üöÄ Modelo Recomendado: NVIDIA Nemotron

**`nvidia/NV-Embed-v2`** es el modelo recomendado por:

- üèÜ **Top performance** en benchmarks de retrieval
- üìä **4096 dimensiones** (alta capacidad de representaci√≥n)
- üìñ **32K tokens** de contexto (documentos largos)
- üéØ **Optimizado para RAG** espec√≠ficamente
- üî¨ **Excelente en contenido t√©cnico/cient√≠fico**

## üì¶ Instalaci√≥n

```bash
# Instalar dependencias
pip install sentence-transformers torch numpy tqdm

# Para GPU (recomendado)
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121

# Opcional: OpenAI
pip install openai
```

## üéì Uso B√°sico

### Ejemplo 1: Embeddings simples

```python
from embedding_generator import EmbeddingGenerator

# Crear generador con Nemotron
generator = EmbeddingGenerator(
    model_name="nemotron-v2",
    device="auto"  # Usa GPU si est√° disponible
)

# Generar embedding para un texto
text = "Wind turbine blade design optimization"
embedding = generator.generate_embedding(text)

print(f"Dimensi√≥n: {len(embedding)}")  # 4096
```

### Ejemplo 2: Procesar chunks

```python
# Procesar directorio completo de chunks
metadata = generator.process_chunks_directory(
    chunks_dir="output_simple/NREL5MW_Reduced/chunks_json",
    output_dir="output_rag/embeddings",
    text_field="content",
    save_format="both"  # JSON + NumPy
)

print(f"Procesados: {metadata['num_chunks']} chunks")
print(f"Tiempo: {metadata['generation_time_seconds']:.2f}s")
```

### Ejemplo 3: Batch de embeddings

```python
textos = [
    "Wind turbine aerodynamics",
    "Offshore wind energy",
    "Power curve optimization"
]

# Generar todos a la vez (m√°s eficiente)
embeddings = generator.generate_embeddings_batch(textos)
print(embeddings.shape)  # (3, 4096)
```

## üéØ Modelos Disponibles

| Alias | Modelo | Dims | Mejor Para | Velocidad |
|-------|--------|------|------------|-----------|
| `nemotron-v2` | nvidia/NV-Embed-v2 | 4096 | RAG t√©cnico/cient√≠fico | ‚≠ê‚≠ê‚≠ê |
| `nemotron-v1` | nvidia/NV-Embed-v1 | 4096 | RAG general | ‚≠ê‚≠ê‚≠ê |
| `bge-large` | BAAI/bge-large-en-v1.5 | 1024 | M√°xima precisi√≥n | ‚≠ê‚≠ê‚≠ê |
| `bge-base` | BAAI/bge-base-en-v1.5 | 768 | Retrieval de calidad | ‚≠ê‚≠ê‚≠ê‚≠ê |
| `bge-small` | BAAI/bge-small-en-v1.5 | 384 | Retrieval r√°pido | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| `mpnet` | all-mpnet-base-v2 | 768 | Alta calidad general | ‚≠ê‚≠ê‚≠ê‚≠ê |
| `minilm` | all-MiniLM-L6-v2 | 384 | Balance velocidad/calidad | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| `multilingual` | paraphrase-multilingual-MiniLM-L12-v2 | 384 | Multiling√ºe | ‚≠ê‚≠ê‚≠ê‚≠ê |
| `openai-small` | text-embedding-3-small | 1536 | API (econ√≥mico) | API |
| `openai-large` | text-embedding-3-large | 3072 | API (mejor calidad) | API |

### Comparar modelos

```python
# Listar todos los modelos
modelos = EmbeddingGenerator.list_available_models()
for alias, nombre in modelos.items():
    print(f"{alias}: {nombre}")

# Ver informaci√≥n detallada
info = EmbeddingGenerator.get_model_info("nemotron-v2")
print(info)
```

## üîß Configuraci√≥n Avanzada

### Personalizar configuraci√≥n

```python
generator = EmbeddingGenerator(
    model_name="nemotron-v2",
    device="cuda",              # Forzar GPU
    batch_size=16,              # Ajustar seg√∫n VRAM
    normalize_embeddings=True,  # Para cosine similarity
    show_progress=True          # Mostrar barra de progreso
)
```

### Usar con OpenAI

```python
import os
os.environ["OPENAI_API_KEY"] = "tu-api-key"

generator = EmbeddingGenerator(
    model_name="openai-small",
    batch_size=100  # OpenAI permite batches grandes
)
```

## üíæ Formatos de Salida

### Formato JSON (con chunks)

```json
{
  "chunk_id": 0,
  "content": "Wind turbine blade design...",
  "metadata": {...},
  "embedding": [0.123, -0.456, ...],
  "embedding_model": "nvidia/NV-Embed-v2"
}
```

### Formato NumPy (matriz de embeddings)

```python
import numpy as np

# Cargar embeddings
embeddings = np.load("output_rag/embeddings/embeddings.npy")
print(embeddings.shape)  # (n_chunks, embedding_dim)
```

### Metadata

```json
{
  "model": "nvidia/NV-Embed-v2",
  "model_alias": "nemotron-v2",
  "embedding_dimension": 4096,
  "num_chunks": 24,
  "normalized": true,
  "generation_time_seconds": 45.23,
  "backend": "sentence-transformers",
  "timestamp": "2026-01-02 15:30:00"
}
```

## üñ•Ô∏è L√≠nea de Comandos

```bash
# Uso b√°sico
python embedding_generator.py output_simple/NREL5MW_Reduced/chunks_json

# Con opciones
python embedding_generator.py \
  output_simple/NREL5MW_Reduced/chunks_json \
  --model nemotron-v2 \
  --output-dir output_rag/embeddings \
  --batch-size 16 \
  --device cuda \
  --save-format both

# Listar modelos disponibles
python embedding_generator.py --list-models

# Ver ayuda
python embedding_generator.py --help
```

### Opciones disponibles

| Opci√≥n | Descripci√≥n | Default |
|--------|-------------|---------|
| `--model` | Modelo a usar | `nemotron-v2` |
| `--output-dir` | Directorio de salida | `chunks_dir/embeddings` |
| `--batch-size` | Tama√±o del batch | `32` |
| `--device` | Dispositivo (auto/cuda/cpu) | `auto` |
| `--save-format` | Formato (json/npy/both) | `both` |
| `--text-field` | Campo JSON con texto | `content` |
| `--list-models` | Listar modelos y salir | - |

## üìä Ejemplos Pr√°cticos

### Ejecutar ejemplos interactivos

```bash
# Men√∫ interactivo
python ejemplos_embeddings.py

# Ejemplo espec√≠fico
python ejemplos_embeddings.py 2  # Procesar chunks
```

### Ejemplos incluidos

1. **Embeddings b√°sico**: Generar embedding para un texto
2. **Procesar chunks**: Procesar directorio completo
3. **Comparar modelos**: Probar diferentes modelos
4. **Batch de embeddings**: M√∫ltiples textos a la vez
5. **Listar modelos**: Ver todos los modelos disponibles
6. **Configuraci√≥n avanzada**: Opciones personalizadas

## üéØ Recomendaciones por Uso

### Para documentos t√©cnicos (tu caso)
```python
generator = EmbeddingGenerator("nemotron-v2")  # ‚≠ê Mejor opci√≥n
```

### Para velocidad m√°xima
```python
generator = EmbeddingGenerator("minilm", batch_size=64)
```

### Para m√°xima precisi√≥n
```python
generator = EmbeddingGenerator("bge-large", batch_size=8)
```

### Para multiling√ºe
```python
generator = EmbeddingGenerator("multilingual")
```

## üìà Performance

Tiempos aproximados para 24 chunks (~150K tokens) en GPU NVIDIA RTX 3080:

| Modelo | Tiempo | Tokens/seg | Dims |
|--------|--------|------------|------|
| nemotron-v2 | ~45s | 3.3K | 4096 |
| bge-large | ~25s | 6.0K | 1024 |
| bge-base | ~15s | 10.0K | 768 |
| minilm | ~8s | 18.7K | 384 |

## üîç Verificar Instalaci√≥n

```python
from embedding_generator import EmbeddingGenerator

# Verificar que funciona
generator = EmbeddingGenerator("minilm")
embedding = generator.generate_embedding("test")
print(f"‚úì Funcionando! Dimensi√≥n: {len(embedding)}")
```

## ü§î Preguntas Frecuentes

**P: ¬øQu√© modelo debo usar?**  
R: Para RAG con documentos t√©cnicos, usa `nemotron-v2`. Para velocidad, usa `minilm`.

**P: ¬øGPU es necesaria?**  
R: No es obligatoria, pero acelera mucho (10-50x m√°s r√°pido).

**P: ¬øLos embeddings ocupan mucho espacio?**  
R: Depende del modelo. Para 24 chunks:
- `minilm` (384 dims): ~37KB
- `nemotron-v2` (4096 dims): ~394KB

**P: ¬øPuedo usar mi propio modelo?**  
R: S√≠, pasa el nombre completo del modelo HuggingFace:
```python
generator = EmbeddingGenerator("tu-usuario/tu-modelo")
```

**P: ¬øC√≥mo s√© si est√° usando GPU?**  
R: Mira la salida al inicializar:
```
‚úì Modelo cargado: nvidia/NV-Embed-v2
  Dispositivo: cuda:0  ‚Üê GPU en uso
```

## üîó Integraci√≥n con RAG

Este m√≥dulo es el primer paso de tu pipeline RAG:

```
1. document_chunker.py    ‚Üí Dividir documento
2. embedding_generator.py ‚Üí Generar embeddings ‚Üê EST√ÅS AQU√ç
3. vector_store.py        ‚Üí Almacenar en base vectorial
4. retriever.py           ‚Üí Buscar chunks relevantes
5. llm_generator.py       ‚Üí Generar respuestas
```

## üìù Siguiente Paso

Ahora que tienes los embeddings, el siguiente paso es crear una base de datos vectorial para b√∫squedas eficientes. Opciones:

- **FAISS** (local, r√°pido)
- **ChromaDB** (local, persistente)
- **Pinecone** (cloud, escalable)
- **Weaviate** (open source, full-featured)

---

**Creado**: 2026-01-02  
**Versi√≥n**: 1.0  
**Autor**: Sistema RAG
